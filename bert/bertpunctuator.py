# -*- coding: utf-8 -*-
"""BertPunctuator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/191TmDhyzcrfzr77G3_-xe2zxcQVR8j8_
"""

from collections import Counter
import re 
from collections import defaultdict
from spacy.matcher import Matcher
from spacy.tokens import Span
from tqdm import tqdm
import spacy
from spacy.tokens import Doc
from spacy.training import Example
from tqdm.notebook import trange
import numpy as np
from spacy.scorer import Scorer
import os
import pandas as pd
import argparse 
import wandb
from simpletransformers.ner import NERModel, NERArgs
import torch
from zipfile import ZipFile
import requests 


parser = argparse.ArgumentParser(description='Process dataframe data.')

parser.add_argument('--output_file',
                    help='output filename')

parser.add_argument('--dataset',
                    help='Files must be a dataframe with headers sentence_id,words,label') 

parser.add_argument('--uncased', action='store_true', default=False,
                    help='Files must be a dataframe with headers sentence_id,words,label') 

parser.add_argument('--bert_model', default="neuralmind/bert-base-portuguese-cased",
                    help='It must one of such models valid bert model, see hugginface plataform.') 



args = parser.parse_args()

case =  'uncased' if args.uncased  else 'cased'

url = f'https://www.dropbox.com/s/awl5c8h7c0x5zjj/punct-dts-{case}.zip?dl=1'
response = requests.get(url, stream=True)

with open(f"punct-dts-{case}.zip", "wb") as handle:
    for data in tqdm(response.iter_content()):
        handle.write(data)


zipfile = ZipFile(os.path.join('./',f'punct-dts-{case}.zip'))
zipfile.extractall(path='./')

zipfile = ZipFile(os.path.join('./',f'punct-{args.dataset}.zip'))
zipfile.extractall(path='./')


DATASET_NAME = os.path.split(args.dataset)[-1]
BASE_DIR = args.dataset

dataset = {
    
}

for filename in os.listdir(BASE_DIR):
  dataset[filename.replace('.csv', '')] = pd.read_csv(os.path.join(BASE_DIR, filename)).dropna()



wandb.login(key='8e593ae9d0788bae2e0a84d07de0e76f5cf3dcf4')



# Create a new run
project = "bert-base-punct"
# Connect an Artifact to the run
model_name = args.bert_model


# Download model weights to a folder and return the path
#model_dir = my_model_artifact.download()
train_args = {
    'evaluate_during_training': True,
    'logging_steps': 10,
    'num_train_epochs': 12,
    'evaluate_during_training_steps': dataset['train'].shape[0],
    'train_batch_size': 12,
    'eval_batch_size': 8,
    'overwrite_output_dir': True,
    'save_eval_checkpoints':False,
    'save_model_every_epoch':False,
    'save_steps':-1,
    'labels_list':dataset['train'].labels.unique().tolist(),
    'use_early_stopping':True,
    'wandb_project':project,
    'wandb_kwargs':{'name': DATASET_NAME+'-'+case}
}


model = NERModel(
    "bert",
    model_name,
    args=train_args,
    use_cuda=torch.cuda.is_available()
)
model.train_model(dataset['train'], eval_data=dataset['dev'])
result, model_outputs, wrong_preds = model.eval_model(dataset['test'])

pd.DataFrame.from_dict(result, orient='index').T.to_csv('overall_model_result.csv', index=False,index_label=False)



nlp = spacy.blank('pt')

def preprocess_data(dataframe):
     

    TOTAL = len(list(dataframe.groupby("sentence_id")))
    data = []
    for _, group in tqdm(dataframe.groupby("sentence_id"), total=TOTAL):
        text = " ".join(group.words)
        doc = nlp.make_doc(text)
        ents = []
        count_dict = defaultdict(lambda : 0)
        for i, label in enumerate(group.labels):
            if label != "O":
                matching_word = group.words.tolist()[i]
                span = Span(doc, i, i+1, label=label)
                ents.append((span.start_char, span.end_char, label.replace('I-','')))
                count_dict[matching_word] += 1
        ent = (text, {
            'entities': ents
        })
        data.append(ent)
    return data

TEST_DATA = preprocess_data(dataset['test'])

model_args = NERArgs()
model_args.labels_list = ["O", "COMMA", "PERIOD", "QUESTION"]

path_to_model = "./outputs/best_model/"
model = NERModel(
    "bert",
    path_to_model,
    args=model_args,
    use_cuda=torch.cuda.is_available()
)

y_true = []
texts = []
for _, group in dataset['test'].groupby("sentence_id"):
        text = " ".join(group.words)
        texts.append(text)
        y_true.append(group.labels.apply(lambda label: label.replace("I-","") ).tolist())

predictions = model.predict(texts)

y_pred = []
for i,pred in enumerate(predictions[0], 1):
  y_pred.append(list(map(lambda item:list(item.values())[0].replace("I-",""), pred)))

text = "Eu gostaria de ir para casa agora mas eu n√£o sei como"


from spacy.tokens import Span

predictions_ner = predictions[0]
doc = nlp(texts[0])
doc.set_ents([Span(doc, i, i+1, "I-"+list(item.values())[0]) 
               for i, item in enumerate(predictions_ner[0]) 
                if list(item.values())[0] !="O"])
ents = list(doc.ents)
ents[0].label_

doc[:2]

examples = []
for i, (text, entities) in enumerate(TEST_DATA):

    doc = nlp(text)
    doc.set_ents([Span(doc, i, i+1, list(item.values())[0]) 
               for i, item in enumerate(predictions_ner[i]) 
                if list(item.values())[0] !="O"])
  
    example = Example.from_dict(doc, entities)
    examples.append(example)




scorer = Scorer(nlp)
scores = Scorer.score_spans(examples, "ents")
print(scores["ents_f"])

ents_per_type = scores.pop('ents_per_type')
pd.DataFrame(ents_per_type).to_csv(os.path.join(path_to_model, 'tst_ents_per_type.csv'),
 index=False, index_label=False)

pd.DataFrame.from_dict(scores, orient='index').T.to_csv(os.path.join(path_to_model, 'tst_scores.csv'), index=False, index_label=False)


with ZipFile(f'bert-base-{DATASET_NAME}') as zipfile:
    zipfile.write(path_to_model)