{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "PreperDataset - Punctuation.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "absFg6SQKdEf"
   },
   "outputs": [],
   "source": [
    "!cp '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Pontuação/ted-2012-03.tgz' ted-2012-03.tgz\n",
    "\n",
    "!tar -xvzf ted-2012-03.tgz"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!tar -xvzf /content/2012-03/texts/pt-br/en/pt-br-en.tgz"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QA9KsoAqgXmD",
    "outputId": "27d3b772-eecf-43f5-d963-07cdd27cb7ab"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pt-br-en/\n",
      "pt-br-en/IWSLT12.TALK.tst2010.pt-br-en.en.xml\n",
      "pt-br-en/train.en\n",
      "pt-br-en/README\n",
      "pt-br-en/IWSLT12.TALK.dev2010.pt-br-en.en.xml\n",
      "pt-br-en/train.tags.pt-br-en.pt-br\n",
      "pt-br-en/IWSLT12.TALK.tst2010.pt-br-en.pt-br.xml\n",
      "pt-br-en/IWSLT12.TALK.dev2010.pt-br-en.pt-br.xml\n",
      "pt-br-en/train.tags.pt-br-en.en\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "try:\n",
    "  os.makedirs('tedtalk2012-03/ptbr')\n",
    "except OSError:\n",
    "  pass \n",
    "\n",
    "for file in os.listdir(\"./pt-br-en\"):\n",
    "    if file.endswith(\".pt-br\") or file.endswith('.pt-br.xml'):\n",
    "        with open(os.path.join(\"./pt-br-en\", file)) as f:    \n",
    "            bs_content = bs(f.read(), \"lxml\")\n",
    "\n",
    "            \n",
    "            transcript = bs_content.find_all('transcript')\n",
    "            \n",
    "            dts = file.split('.')[-4]\n",
    "            if not transcript:\n",
    "\n",
    "              segs = bs_content.find_all('seg')\n",
    "              with open(f'./tedtalk2012-03/ptbr/tedtalk2012.{dts}', 'w') as f:\n",
    "                  for seg in segs:\n",
    "                    f.write(seg.get_text().strip()+'\\n')\n",
    "            else:\n",
    "\n",
    "              with open(f'./tedtalk2012-03/ptbr/tedtalk2012.{dts}', 'w') as f:\n",
    "                  for transp in transcript:\n",
    "                    f.write(transp.get_text().strip()+'\\n')\n"
   ],
   "metadata": {
    "id": "KE66ga5drhG2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!cp '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Corpus de Tipos Textuais Brasileiros/corpus-ttbr-v0.1.6-unbalanced.zip' corpus-ttbr-v0.1.6-unbalanced.zip\n",
    "!unzip corpus-ttbr-v0.1.6-unbalanced.zip\n"
   ],
   "metadata": {
    "id": "ei9hAUZglMtk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ],
   "metadata": {
    "id": "YPoOvTO-7gnG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Here is an example:\n",
    "\n",
    "https://www.clips.uantwerpen.be/conll2003/ner/"
   ],
   "metadata": {
    "id": "646CRd9pr-J3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import regexp\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def replace(sent_id, sentence):\n",
    " \n",
    "  \n",
    "  tokenizer = regexp.RegexpTokenizer('\\w+|[.,?]')\n",
    "\n",
    "  #we lowercasedthe entire corpus with the purpose of eliminating bias \n",
    "  #around the prediction ofperiods.\n",
    "  #Automatic punctuation restoration with BERT models #Nagy et. al\n",
    "\n",
    "\n",
    "  tokens = tokenizer.tokenize(sentence.lower())\n",
    "  sent_data = []\n",
    "  for i,token in enumerate(tokens):\n",
    "    try:\n",
    "      if token not in ['.', ',', '?']:\n",
    "        sent_data.append([sent_id,'O',token])\n",
    "      elif token == '.':\n",
    "        sent_data[-1][1] = 'I-PERIOD' \n",
    "        \n",
    "      elif token == ',':  \n",
    "        sent_data[-1][1] = 'I-COMMA' \n",
    "       \n",
    "      elif token == '?':\n",
    "       sent_data[-1][1] = 'I-QUESTION'\n",
    "    except IndexError:\n",
    "      continue\n",
    "  \n",
    "  return sent_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TED Talk dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -r ./tedtalk2012-03/ptbr/.ipynb_checkpoints\n",
    "!rm -r ./tedtalk2012/.ipynb_checkpoints\n",
    "!rm -r ./tedtalk2012/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from itertools import chain\n",
    "sent_tokenize('Olá Mundo 1. Olá mundo 2')\n",
    "\n",
    "\n",
    "sents_ = list(chain.from_iterable(map(sent_tokenize, ['Olá Mundo 1. Olá mundo 2']*2)))\n",
    "words_ = list(chain.from_iterable(map(word_tokenize,sents_)))\n",
    "\n",
    "\n",
    "words_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_df(df):\n",
    "  sents_ids_uniq = df.sentence_id.astype(np.int32).unique()\n",
    "  # TEDTALK proproportion\n",
    "  trainIdx, testIdx = train_test_split(\n",
    "      sents_ids_uniq, test_size=0.017593607011664625, random_state=42)\n",
    "\n",
    "  testIdx, devIdx = train_test_split(\n",
    "      testIdx, test_size=0.36100936100936104, random_state=42)\n",
    "  \n",
    "  sents_id = df['sentence_id'].astype(np.int32).to_numpy()\n",
    "\n",
    "  train_df = df[np.isin(sents_id, trainIdx )]\n",
    "  test_df = df[np.isin(sents_id, testIdx )]\n",
    "  dev_df = df[np.isin(sents_id, devIdx )]\n",
    "\n",
    "  return train_df, dev_df, test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_DIR = './tedtalk2012-03/ptbr/'\n",
    "\n",
    "PATH_TO_SAVE = './tedtalk2012'\n",
    "\n",
    "try:\n",
    "  os.mkdir(PATH_TO_SAVE)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "dict_tedtalk = {\n",
    "    'dev2010':'dev',\n",
    "    'train':'train',\n",
    "    'tst2010':'test'\n",
    "}\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "text_dts = []\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "\n",
    "  file = open(os.path.join(BASE_DIR, filename), encoding='utf-8')\n",
    "  data = file.readlines()\n",
    "  dataset = []\n",
    "  i = 0\n",
    "  \n",
    "  for line in data:\n",
    "    text = re.sub(r'[!;]', '.', line)\n",
    "    text = re.sub(r'[:]', ',', text)\n",
    "    text = re.sub(r'\\s[-]\\s',',',text).lower()\n",
    "\n",
    "    emotions = re.findall('\\(\\w+\\)', text)\n",
    "    if len(emotions) > 0:\n",
    "      continue\n",
    "    text_dts.append(text)\n",
    "    dataset.extend(replace(i,text))\n",
    "    i += 1\n",
    "   \n",
    "  df = pd.DataFrame(np.array(dataset), columns=['sentence_id', 'labels', 'words'])\n",
    "  print(filename)\n",
    "  file_save = dict_tedtalk[filename.split('.')[-1]] \n",
    "  \n",
    "  df.to_csv(os.path.join(PATH_TO_SAVE, file_save+'.csv'), index=False, index_label=False)\n",
    "  dfs[file_save] = df\n",
    "  \n",
    "   \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev_size = len(list(dfs['dev'].groupby('sentence_id')))\n",
    "tst_size = len(list(dfs['test'].groupby('sentence_id')))\n",
    "train_size = len(list(dfs['train'].groupby('sentence_id')))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tedtalk = pd.concat(dfs)\n",
    "tedtalk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sents_ = list(chain.from_iterable(map(sent_tokenize, text_dts)))\n",
    "\n",
    "\n",
    "info_dict = {\n",
    "    'tedtalk2012': {\n",
    "        'num_texts':len(text_dts),\n",
    "        'num_sents':len(sents_),\n",
    "        'num_words':tedtalk.shape[0]\n",
    "    },\n",
    "    'obras': {\n",
    "        'num_texts':0,\n",
    "        'num_sents':0,\n",
    "        'num_words':0\n",
    "    },\n",
    "  \n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_tedtalk = pd.concat(dfs)\n",
    "#tedtalk_train_df,tedtalk_dev_df,tedtalk_test_df =  split_df(df_tedtalk)\n",
    "tedtalk_train_df.to_csv(f'{PATH_TO_SAVE}/train.csv', index=False, index_label=False)\n",
    "tedtalk_dev_df.to_csv(f'{PATH_TO_SAVE}/dev.csv', index=False, index_label=False)\n",
    "tedtalk_test_df.to_csv(f'{PATH_TO_SAVE}/test.csv', index=False, index_label=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!zip punct-tedtalk2012.zip -r tedtalk2012/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp punct-tedtalk2012.zip '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Pontuação/punct-tedtalk2012.zip'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obras Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_DIR = 'corpus-ttbr-v0.1.6-unbalanced/narrative'\n",
    "\n",
    "i = 0\n",
    "dataset2 = []\n",
    "text_dts_obras = []\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "  file = open(os.path.join(BASE_DIR, filename))\n",
    "  data = file.readlines()\n",
    "  \n",
    "  for line in data:\n",
    "    \n",
    "      text = re.sub(r'[!;]', '.', line)\n",
    "      text = re.sub(r'[:]', ',', text)\n",
    "      text = re.sub(r'\\s[-]\\s',',',text).lower()\n",
    "\n",
    "      emotions = re.findall('\\(\\w+\\)', text)\n",
    "      if len(emotions) > 0:\n",
    "        continue\n",
    "      text_dts_obras.append(text)\n",
    "      dataset2.extend(replace(i,text))\n",
    "      i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obras_df = pd.DataFrame(np.array(dataset2), columns=['sentence_id', 'labels', 'words'])\n",
    "sents_ = list(chain.from_iterable(map(sent_tokenize, text_dts_obras)))\n",
    "info_dict['obras']['num_sents'] = len(sents_)\n",
    "info_dict['obras']['num_texts'] = len(text_dts_obras)\n",
    "info_dict['obras']['num_words'] = obras_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obras_train_df, obras_dev_df, obras_test_df = split_df(obras_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obras_train_df.shape, obras_test_df.shape, obras_dev_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obras_df.labels.value_counts().to_latex()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('output.xlsx') as writer:  \n",
    "    obras_df.labels.value_counts().to_excel(writer, sheet_name='obras')\n",
    "    tedtalk.labels.value_counts().to_excel(writer, sheet_name='tedtalk')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp output.xlsx './drive/MyDrive/output.xlsx'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -r obras/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(info_dict).to_latex()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_SAVE = 'obras/'\n",
    "try:\n",
    "  os.mkdir(PATH_TO_SAVE)\n",
    "except FileExistsError:\n",
    "  pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obras_train_df.to_csv(f'{PATH_TO_SAVE}/train.csv', index=False, index_label=False)\n",
    "obras_dev_df.to_csv(f'{PATH_TO_SAVE}/dev.csv', index=False, index_label=False)\n",
    "obras_test_df.to_csv(f'{PATH_TO_SAVE}/test.csv', index=False, index_label=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tedtalk_train_df = dfs['train']\n",
    "tedtalk_dev_df = dfs['dev']\n",
    "tedtalk_test_df = dfs['test']\n",
    "\n",
    "df_info_split = pd.DataFrame({\n",
    "    'tedtalk2012': {\n",
    "        'train':tedtalk_train_df.sentence_id.unique().shape[0],\n",
    "        'dev':tedtalk_dev_df.sentence_id.unique().shape[0],\n",
    "        'test':tedtalk_test_df.sentence_id.unique().shape[0]\n",
    "    },\n",
    "    'obras':{\n",
    "        'train':obras_train_df.sentence_id.unique().shape[0],\n",
    "        'dev':obras_dev_df.sentence_id.unique().shape[0],\n",
    "        'test':obras_test_df.sentence_id.unique().shape[0]\n",
    "    }\n",
    "})\n",
    "df_info_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perct_test = df_info_split['tedtalk2012']['test']/pd.concat(list(dfs.values())).sentence_id.unique().shape[0]\n",
    "perct_test += df_info_split['tedtalk2012']['dev']/pd.concat(list(dfs.values())).sentence_id.unique().shape[0]\n",
    "perct_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "0.017593607011664625*100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perct_test = df_info_split['tedtalk2012']['test']/pd.concat(list(dfs.values())).sentence_id.unique().shape[0]\n",
    "perct_dev = df_info_split['tedtalk2012']['dev']/pd.concat(list(dfs.values())).sentence_id.unique().shape[0]\n",
    "\n",
    "perct_dev/(perct_test+perct_dev)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tedtalk_train_df.sentence_id.unique().shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_info_split.to_latex()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!zip punct-obras.zip -r obras/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp punct-obras.zip '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Pontuação/punct-obras.zip'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "datasets = ['obras', 'tedtalk2012']\n",
    "PATH_TO_SAVE = 'punct-all/'\n",
    "\n",
    "try:\n",
    "  os.mkdir(PATH_TO_SAVE)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "\n",
    "for dtype in ['train', 'dev', 'test']:\n",
    "  dfs = []\n",
    "  for dt in datasets:\n",
    "     print(os.path.join(dtype, dt))\n",
    "     dfs.append(pd.read_csv(os.path.join(dt,dtype+'.csv' )))\n",
    "\n",
    "  new_df = pd.concat(dfs)\n",
    "  new_df.to_csv(os.path.join(PATH_TO_SAVE, dtype)+'.csv', index=False, index_label=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!zip punct-all.zip -r punct-all/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('punct-all', 'zip', 'punct-all/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive('punct-all.zip', 'extracted')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!zip punct-dts.zip -l punct-all.zip punct-obras.zip punct-tedtalk2012.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!unzip -l punct-dts.zip "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp punct-dts-cased.zip '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Pontuação/punct-dts-cased.zip'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git config user.name \"Tiagoblima\"\n",
    "!git config user.email \"tiago.blima02@gmail.com\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://Tiagoblima:n9659M0628@github.com/mec-correcaotextual/punctuation-restauration-bracis2022.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('metadata_test_final.csv')\n",
    "\n",
    "for txt in df.task:\n",
    "  print(txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.task"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gdown"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gdown --id 1ujlfIl7iN-0HJ2vAtbZGFbP43u-NBFav"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!unzip TTS-Portuguese-Corpus_22khz.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lines = open('/content/TTS-Portuguese-Corpus_22khz/texts.csv').readlines()\n",
    "\n",
    "texts = list(map(lambda txt: ' '.join(txt.split('==')[1].strip('\\n').split()), lines))\n",
    "texts[:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('texts.txt') as f:\n",
    "  for txt in texts:\n",
    "    f.write(txt+'\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_DIR = './texts/'\n",
    "\n",
    "i = 0\n",
    "dataset2 = []\n",
    "text_dts_obras = []\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "  file = open(os.path.join(BASE_DIR, filename))\n",
    "  data = file.readlines()\n",
    "  \n",
    "  for line in data:\n",
    "    \n",
    "      text = re.sub(r'[!;]', '.', line)\n",
    "      text = re.sub(r'[:]', ',', text)\n",
    "      text = re.sub(r'\\s[-]\\s',',',text).lower()\n",
    "\n",
    "      emotions = re.findall('\\(\\w+\\)', text)\n",
    "      if len(emotions) > 0:\n",
    "        continue\n",
    "      text_dts_obras.append(text)\n",
    "      dataset2.extend(replace(i,text))\n",
    "      i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref = https://github.com/Edresson/TTS-Portuguese-Corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset2,  columns=['sentence_id', 'labels', 'words'])\n",
    "\n",
    "df.to_csv('tts-portuguese-corpus.csv', index=False,index_label=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive('texts.zip', 'texts')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "datasets = ['obras', 'tedtalk2012']\n",
    "PATH_TO_SAVE = 'punct-all/'\n",
    "\n",
    "try:\n",
    "  os.mkdir(PATH_TO_SAVE)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "\n",
    "for dtype in ['train', 'dev', 'test']:\n",
    "  dfs = []\n",
    "  for dt in datasets:\n",
    "     print(os.path.join(dtype, dt))\n",
    "     dfs.append(pd.read_csv(os.path.join(dt,dtype+'.csv' )))\n",
    "\n",
    "  new_df = pd.concat(dfs)\n",
    "  new_df.to_csv(os.path.join(PATH_TO_SAVE, dtype)+'.csv', index=False, index_label=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aheq9IVkRQrP",
    "outputId": "4e975d5c-ce1d-4a02-a8ba-e8bd102d0014"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train/obras\n",
      "train/tedtalk2012\n",
      "dev/obras\n",
      "dev/tedtalk2012\n",
      "test/obras\n",
      "test/tedtalk2012\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!zip punct-all.zip -r punct-all/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOi_YXHwMOVe",
    "outputId": "716d3b97-6958-40ca-baa8-9a6120fbeffd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: punct-all/ (stored 0%)\n",
      "  adding: punct-all/train.csv (deflated 73%)\n",
      "  adding: punct-all/test.csv (deflated 72%)\n",
      "  adding: punct-all/dev.csv (deflated 71%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "shutil.make_archive('punct-all', 'zip', 'punct-all/')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vB3lVN1Kje-d",
    "outputId": "3a9427fd-2728-4cd4-8b3b-7d1e203bbed8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/punct-all.zip'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 46
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive('punct-all.zip', 'extracted')"
   ],
   "metadata": {
    "id": "Yhm_093cjoz2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!zip punct-dts.zip -l punct-all.zip punct-obras.zip punct-tedtalk2012.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PB-eDg8K28I",
    "outputId": "0feb83a6-15d8-4d37-dbe4-8f553776f7d3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: punct-all.zip (stored 0%)\n",
      "  adding: punct-obras.zip (stored 0%)\n",
      "  adding: punct-tedtalk2012.zip (stored 0%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -l punct-dts.zip "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5apQESdLYdY",
    "outputId": "dea3efab-db18-4651-d3a0-9bbbeac04876"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unzip:  cannot find or open punct-dts.zip, punct-dts.zip.zip or punct-dts.zip.ZIP.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!cp punct-dts-cased.zip '/content/drive/Shareddrives/MEC - Correção textual/PLN/Dataset/Pontuação/punct-dts-cased.zip'"
   ],
   "metadata": {
    "id": "ADVcMhpSMZql"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!git config user.name \"Tiagoblima\"\n",
    "!git config user.email \"tiago.blima02@gmail.com\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx_cr4bzdDcn",
    "outputId": "2eee246f-1c40-4e0f-c5fc-0353c26f9c49"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: not in a git directory\n",
      "fatal: not in a git directory\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://Tiagoblima:n9659M0628@github.com/mec-correcaotextual/punctuation-restauration-bracis2022.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajrG4R8zchMh",
    "outputId": "b805c722-bf1e-407d-e5b1-4af6c50245e8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'punctuation-restauration-bracis2022'...\n",
      "remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.\n",
      "remote: Please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.\n",
      "fatal: Authentication failed for 'https://Tiagoblima:n9659M0628@github.com/mec-correcaotextual/punctuation-restauration-bracis2022.git/'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('metadata_test_final.csv')\n",
    "\n",
    "for txt in df.task:\n",
    "  print(txt)"
   ],
   "metadata": {
    "id": "DRAK0zLmzb90"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.task"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW_0kxwq0D9i",
    "outputId": "02b705e8-6e4b-4499-8a94-6d6f3114bcc4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        annotation_and_transcription\n",
       "1        annotation_and_transcription\n",
       "2        annotation_and_transcription\n",
       "3        annotation_and_transcription\n",
       "4        annotation_and_transcription\n",
       "                     ...             \n",
       "12671                   transcription\n",
       "12672                   transcription\n",
       "12673                   transcription\n",
       "12674                   transcription\n",
       "12675                   transcription\n",
       "Name: task, Length: 12676, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gdown"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UthTBa560UDA",
    "outputId": "9663dc2d-8a4b-4199-ab5d-aa01d8cac2d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown --id 1ujlfIl7iN-0HJ2vAtbZGFbP43u-NBFav"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHl6zXbs0V8g",
    "outputId": "92275c6a-4e3e-48f4-a6a8-4cb5d4d53a55"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ujlfIl7iN-0HJ2vAtbZGFbP43u-NBFav\n",
      "To: /content/TTS-Portuguese-Corpus_22khz.zip\n",
      "100% 2.91G/2.91G [00:26<00:00, 112MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip TTS-Portuguese-Corpus_22khz.zip"
   ],
   "metadata": {
    "id": "RiLCKbK80Zvz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lines = open('/content/TTS-Portuguese-Corpus_22khz/texts.csv').readlines()\n",
    "\n",
    "texts = list(map(lambda txt: ' '.join(txt.split('==')[1].strip('\\n').split()), lines))\n",
    "texts[:2]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDgcylC6016A",
    "outputId": "0fc688e9-9510-4033-c6df-fa9c252d9895"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['O título de página solicitado contém carateres inválidos.',\n",
       " 'O academicismo ou academismo designam, originalmente, o método de ensino artístico profissionalizante concebido, formalizado e ministrado pelas academias de arte europeias.']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open('texts.txt') as f:\n",
    "  for txt in texts:\n",
    "    f.write(txt+'\\n')"
   ],
   "metadata": {
    "id": "WtLebZgx3M2N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "BASE_DIR = './texts/'\n",
    "\n",
    "i = 0\n",
    "dataset2 = []\n",
    "text_dts_obras = []\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "  file = open(os.path.join(BASE_DIR, filename))\n",
    "  data = file.readlines()\n",
    "  \n",
    "  for line in data:\n",
    "    \n",
    "      text = re.sub(r'[!;]', '.', line)\n",
    "      text = re.sub(r'[:]', ',', text)\n",
    "      text = re.sub(r'\\s[-]\\s',',',text).lower()\n",
    "\n",
    "      emotions = re.findall('\\(\\w+\\)', text)\n",
    "      if len(emotions) > 0:\n",
    "        continue\n",
    "      text_dts_obras.append(text)\n",
    "      dataset2.extend(replace(i,text))\n",
    "      i += 1"
   ],
   "metadata": {
    "id": "bpYjPRhY37Jv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ref = https://github.com/Edresson/TTS-Portuguese-Corpus"
   ],
   "metadata": {
    "id": "ZI8oxCMQ_eCc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(dataset2,  columns=['sentence_id', 'labels', 'words'])\n",
    "\n",
    "df.to_csv('tts-portuguese-corpus.csv', index=False,index_label=False)"
   ],
   "metadata": {
    "id": "-gYIgye-4KpX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "shutil.unpack_archive('texts.zip', 'texts')"
   ],
   "metadata": {
    "id": "QnlBTT-z-RKU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "FWU74Jlv-gUU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}